{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"KernelSize-Optimization.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM3IB0TcaUGxYfx0lq8Xdl2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"RNXuTPwqMjAQ","colab_type":"code","outputId":"103805bd-01b3-4fc5-fcc3-9d2c95d480c3","executionInfo":{"status":"ok","timestamp":1581627815950,"user_tz":300,"elapsed":6749,"user":{"displayName":"Pedram Khayyatkhoshnevis","photoUrl":"","userId":"14668972162109202832"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","import torch\n","\n","from torch.nn import Conv1d\n","\n","from torch.nn import MaxPool1d\n","\n","from torch.nn import Flatten\n","\n","from torch.nn import Linear\n","\n","from torch.nn.functional import relu\n","\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","from torch.optim import SGD\n","\n","from torch.nn import L1Loss\n","\n","!pip install pytorch-ignite\n","\n","from ignite.contrib.metrics.regression.r2_score import R2Score"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting pytorch-ignite\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/55/41e8a995876fd2ade29bdba0c3efefa38e7d605cb353c70f3173c04928b5/pytorch_ignite-0.3.0-py2.py3-none-any.whl (103kB)\n","\r\u001b[K     |███▏                            | 10kB 24.2MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 40kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 61kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 71kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 81kB 3.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 92kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 3.4MB/s \n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-ignite) (1.4.0)\n","Installing collected packages: pytorch-ignite\n","Successfully installed pytorch-ignite-0.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PW59khdv4PUj","colab_type":"code","colab":{}},"source":["class CnnRegressor(torch.nn.Module):\n","\n","  def __init__(self, batch_size, inputs, outputs, kernelsize):\n","\n","    super(CnnRegressor, self).__init__()\n","    self.batch_size = batch_size\n","    self.inputs = inputs\n","    self.outputs = outputs\n","\n","    self.input_layer = Conv1d(inputs, batch_size, kernelsize)\n","\n","    self.max_pooling_layer = MaxPool1d(1)\n","\n","    self.conv_layer = Conv1d(batch_size, 128, kernelsize)\n","\n","    self.flatten_layer = Flatten()\n","\n","    self.linear_layer = Linear(128, 64)\n","\n","    self.output_layer = Linear(64, outputs)\n","\n","  \n","  def feed(self, input):\n","\n","    input = input.reshape((self.batch_size, self.inputs, 1))\n","\n","    output = relu(self.input_layer(input))\n","\n","    output = self.max_pooling_layer(output)\n","\n","    output = relu(self.conv_layer(output))\n","\n","    output = self.flatten_layer(output)\n","\n","    output = self.linear_layer(output)\n","\n","    output = self.output_layer(output)\n","\n","    return output\n","\n","def model_loss(model, dataset, train = False, optimizer = 'adam'):\n","\n","  performance = L1Loss()\n","  score_metric = R2Score()\n","\n","  avg_loss = 0\n","  avg_score = 0\n","  count = 0\n","\n","  for input, output in iter(dataset):\n","\n","    predictions = model.feed(input)\n","\n","    loss = performance(predictions, output)\n","\n","    score_metric.update([predictions, output])\n","    score = score_metric.compute()\n","        \n","    if(train):\n","\n","      optimizer.zero_grad()\n","\n","      loss.backward()\n","\n","      optimizer.step()\n","\n","    avg_loss += loss.item()\n","    avg_score += score\n","    count += 1\n","\n","  return avg_loss / count, avg_score / count"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dhoAhIINsOuG","colab_type":"code","colab":{}},"source":["dataset = pd.read_csv('housing.csv')\n","\n","dataset = dataset.dropna()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"up3y8A6xstLZ","colab_type":"code","colab":{}},"source":["Y = dataset['median_house_value']\n","X = dataset.loc[:,'longitude':'median_income']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KfO1oOAjtSl5","colab_type":"code","colab":{}},"source":["# 30% for testing dataset\n","__testPercent__ = 0.3\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=__testPercent__)\n","\n","x_train_np = x_train.to_numpy()\n","\n","y_train_np = y_train.to_numpy()\n","\n","x_test_np = x_test.to_numpy()\n","y_test_np = y_test.to_numpy()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0D-cNReMtbLc","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"LdhBwD4G3S-Y","colab_type":"code","outputId":"94b86360-c2a4-4871-a9e2-172a65ee4d45","executionInfo":{"status":"error","timestamp":1581627872186,"user_tz":300,"elapsed":62971,"user":{"displayName":"Pedram Khayyatkhoshnevis","photoUrl":"","userId":"14668972162109202832"}},"colab":{"base_uri":"https://localhost:8080/","height":408}},"source":["ResultArry = {}\n","\n","for x in range(1,10):\n","\n","  # To see and compare the final results\n","  currentID = \"kernel-\"+str(x)\n","\n","  batch_size = 38\n","\n","  model = CnnRegressor(batch_size, X.shape[1], 1,x)\n","\n","  model.cuda()\n","\n","  epochs = 50\n","\n","  optimizer = SGD(model.parameters(), lr=1e-5)\n","  reshapeddata = y_train_np.reshape(y_train_np.shape[0],1)\n","  inputs = torch.from_numpy(x_train_np).cuda().float()\n","  outputs = torch.from_numpy(y_train_np.reshape(y_train_np.shape[0],1)).cuda().float()\n","\n","  tensor = TensorDataset(inputs, outputs)\n","\n","  loader = DataLoader(tensor, batch_size, shuffle=True, drop_last=True)\n","\n","  for epoch in range(epochs):\n","    avg_loss, avg_r2_score = model_loss(model, loader, train=True, optimizer=optimizer)\n","    # print(\"Epoch \"+ str(epoch + 1) + \":\\n\\tLoss = \" + str(avg_loss) + \"\\n\\r R2 score = \" + str(avg_r2_score)  )\n","\n","  inputs = torch.from_numpy(x_test_np).cuda().float()\n","  outputs = torch.from_numpy(y_test_np.reshape(y_test_np.shape[0],1)).cuda().float()\n","\n","  tensor = TensorDataset(inputs, outputs)\n","  loader = DataLoader(tensor, batch_size, shuffle=True, drop_last=True)\n","\n","  avg_loss, avg_r2_score = model_loss(model, loader)\n","  # print(\"the model L1 loss is: \" + str(avg_loss))\n","  # print(\"the model R2 score is: \" + str(avg_r2_score))\n","\n","  ResultArry.update({currentID:avg_r2_score})\n","\n","  print(\"now:\"+str(currentID)+' ++')\n","  print(ResultArry)\n","\n","print(ResultArry)\n","#get max accuracy\n","maxkey=0\n","maxval = -9999999\n","for key in ResultArry:\n","  if(ResultArry[key]>maxval):\n","      maxval = ResultArry[key]\n","      maxkey = key\n","\n","print('best result was for:')\n","print(str(maxkey) + ' -> ' + str(ResultArry[maxkey]))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["now:kernel-1 ++\n","{'kernel-1': 0.3014435231076445}\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-4160abedac96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_r2_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;31m# print(\"Epoch \"+ str(epoch + 1) + \":\\n\\tLoss = \" + str(avg_loss) + \"\\n\\r R2 score = \" + str(avg_r2_score)  )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-7ed826dcd9a3>\u001b[0m in \u001b[0;36mmodel_loss\u001b[0;34m(model, dataset, train, optimizer)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperformance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-7ed826dcd9a3>\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pooling_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    201\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 202\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (1). Kernel size: (2). Kernel size can't be greater than actual input size"]}]}]}